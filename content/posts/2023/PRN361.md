---
title: "[论文阅读笔记 -- VLP] Vision Learners Meet Web Image-Text Pairs (2023)"
date: 2023-03-05T08:34:37+08:00
categories: ["Paper Reading Notes"]
tags: ["paper reading", "cv", "notes", "cross-modal", "vlp"]
draft: false
---

# 2301.07088 Vision Learners Meet Web Image-Text Pairs (2023)

[开源代码传送门](https://huggingface.co/spaces/tennant/MUG_caption)

![Fig 1](/images/2023/PRN361/1.png)

## 概述

可以将先前的表征学习方法分为三类：  
+ Single-modal Discriminative (SimCLR)
+ Single-modal Generative (MAE)
+ Mlti-modal Discriminative (CLIP)

两个发现： 
+ 生成式方法的迁移表现更好
+ 单模态 SSL 方法通常比多模态 SSL 方法表现更好

因此本文设计了第四种范式，即 Multi-modal Generative Pre-training，提出一种多模态生成器 (MUlti-modal Generator, MUG)。  

### 方法对比

![Fig 2](/images/2023/PRN361/2.png)

### 模型架构

![Fig 3](/images/2023/PRN361/3.png)

### 可视化示例

![Fig 4](/images/2023/PRN361/4.png)
