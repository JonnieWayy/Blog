---
title: "[论文阅读笔记 -- MAE] Generic-to-Specific Distillation of Masked Autoencoders (CVPR 2023)"
date: 2023-03-11T18:17:26+08:00
categories: ["Paper Reading Notes"]
tags: ["paper reading", "cv", "nlp", "notes", "MAE", "attention", "konwledge distillation"]
draft: false
---

# Generic-to-Specific Distillation of Masked Autoencoders (CVPR 2023)

[开源代码传送门](https://github.com/pengzhiliang/G2SD)

![Fig 1](/images/2023/PRN369/1.png)

## 概述

本文发现小型 ViT 从大量数据或自监督学习方法中获益有限，提出 General-to-Specific Distillation (G2SD) 从 MAEs 向轻量 ViTs 迁移任务无关与特定知识。  

### 模型架构

![Fig 2](/images/2023/PRN369/2.png)
