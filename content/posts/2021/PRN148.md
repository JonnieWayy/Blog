---
title: "[论文阅读笔记 -- 跨模态检索] Self-Supervised Visual Representations for CMR (ICMR 2019)"
date: 2021-12-29T12:48:11+08:00
categories: ["Paper Reading Notes"]
tags: ["paper reading", "cv", "notes", "cross-modal", "retrieval"]
draft: false
---

# Self-Supervised Visual Representations for Cross-Modal Retrieval (ICMR 2019)

![Fig 2](/images/2021/PRN148/2.png)

## 概述

本文提出一种自监督的跨模态检索架构，利用图文之间的相关性作为监督信号，以学习能够有效迁移到其他 CV 任务上的视觉特征。  

![Fig 3](/images/2021/PRN148/3.png)

### 监督学习与自监督学习

监督学习有人工标签：  

$$R = \sum_{i=1}^N [loss(f(x_{i}, \Theta), y_{i})].$$

自监督学习无需人工标签，训练数据可以被分成多份，互相提供自监督信号：  

$$R = \sum_{i=1}^N [loss(f(x_{i}^{+}, \Theta), x_{i}^{-})].$$

![Fig 1](/images/2021/PRN148/1.png)

### Latent Dirichlet Allocation (LDA)

是一种统计生成模型，可表征为一个三级的层叠贝叶斯模型。  

### 网络架构

这里使用 AlexNet。  

### 可视化示例

![Fig 4](/images/2021/PRN148/4.png)

![Fig 5](/images/2021/PRN148/5.png)
