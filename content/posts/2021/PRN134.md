---
title: "[论文阅读笔记 -- ViT] GroupBERT: Enhanced TF with Efficient Grouped Structures (2021)"
date: 2021-12-02T21:15:17+08:00
categories: ["Paper Reading Notes"]
tags: ["paper reading", "cv", "notes", "attention", "transformer", "ViT", "CNN", "nlp"]
draft: false
---

# 2106.05822 GroupBERT: Enhanced Transformer Architecture with Efficient Grouped Structures (2021)

![Fig 1](/images/2021/PRN134/1.png)

## 概述

本文对 Transformer 曾的结构进行了一些改进：  

+ 增加一个卷积模块作为自注意力模块的补充，分解局部与全局关系的学习
+ 引入 grouped transformeation 以降低前馈层与卷积层的计算复杂度

提出 GroupBERT 模型，主要包含四个模块：  

+ 一个多头注意力模块 (MHA)
+ 一个分组卷积模块
+ 两个分组前馈模块 (GFFN)

各模块均都包含 dropout、跳连以及 layer normalization， GroupBERT 每一层均包含上述四个模块。  

## 特征图可视化
![Fig 2](/images/2021/PRN134/2.png)

## Efficient Parameter Utilization

将 layer normalization 从残差连接之后移动到残差块的开始处虽然不能直接提升表现，但是能够使训练更加稳定，能容许使用更大的学习率。  

此外，本文发现 dropout 的使用对预训练阶段不利，但这种现象并不发生于使用更小规模数据集的下游任务。因此本文只在微调阶段使用 dropout。  

![Fig 3](/images/2021/PRN134/3.png)
