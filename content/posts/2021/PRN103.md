---
title: "[论文阅读笔记 -- ReID] Learning Posterior and Prior for Uncertainty Modeling in ReID(2020)"
date: 2021-09-25T15:30:28+08:00
categories: ["Paper Reading Notes"]
tags: ["paper reading", "cv", "notes", "cross-modal", "retrieval", "ReID"]
draft: false
---

# 2007.08785 Learning Posterior and Prior for Uncertainty Modeling in Person Re-Identification (2020)

## 概述

本文在 ReID 任务中同时学习样本后验与类别先验，以量化输入图像及其相应类别的不确定性。  

![Fig 1](/images/2021/PRN103/1.png)

## 整体架构

上分支用 GAP 处理特征图得到 \\(\mu\_{\phi}(x) \in \mathbb{R}^{C}\\) 表示期望。  

下分支用 \\(\Sigma\\)-Net 处理特征图得到高阶特征 \\(\sigma\_{\phi}(x)\\) 即方差，表征潜在编码 \\(z\\) 在各个相应维度上的不确定性。  

二者联合定义随机变量的后验： \\(z \sim q_{\phi}(z|x) = \mathcal{N}(z; \mu\_{\phi}(x), \sigma\_{\phi}(x))\\)。  

为了完成分类，又定义 \\(z\\) 上的类别先验，有  

$$p(z|y) = \mathcal{N}(z; \mu^{(y)}, \sigma^{(y)}),$$

其中 \\(y\\) 为图像 \\(x\\) 的类别标签，\\(\mu^{(y)}\\) 和 \\(\sigma^{(y)}\\) 是可训练的参数。  

定义了一个分布损失 (distribution loss) 来连结先验与后验。  

## \\(\Sigma\\)-Net

核心观点在于利用高阶特征使得 \\(\sigma\_{\phi}(x)\\) 不同于一阶特征 \\(\mu\_{\phi}(x)\\)。

![Fig 2](/images/2021/PRN103/2.png)

## 分布损失

$$\mathcal{L}\_{cls} = -\frac{1}{N} \sum_{i=1}^{N} \sum_{k=1}^{K} \mathbb{I}(k == y)log(k|z) = -\frac{1}{N} \sum_{i=1}^{N} log \frac{exp(-D_{KL}(q_{\phi}(z,y|x_{i}) || p_{\psi(z, y)}))}{\sum_{k=1}^{K}exp(-D_{KL}(q_{\phi}(z,k|x_{i}) || p_{\psi(z, k)}))},$$
$$\mathcal{L}\_{KL} = D_{KL}(q\_{\phi}(z,y|x) || p\_{\psi}(z, y)),$$
$$\mathcal{L}\_{dist} = \mathcal{L}\_{cls} + \lambda \mathcal{L}\_{KL}.$$

## 先验指导的软标签

不同类别之间的距离并不相同，而这些距离能够帮助分类其理解任务。  

本文所构建的各个类别的先验分布之间的距离可以反映出类别之间的相似度。  

用 Wasserstein 距离 \\(D_{w}\\) 衡量两个类别先验之间的相似度：  

$$D_{w} (p\_{\psi\_{1}}(z|y_{1});p\_{\psi\_{2}}(z|y_{2})) = ||\mu^{(y_{1})} - \mu^{(y_{2})}||\_{2}^{2} + ||\sqrt{\sigma^{(y_{1})}} - \sqrt{\sigma^{(y_{2})}}||\_{F}^{2}.$$

进而归一化：  

$$y_{soft} = \frac{exp(-D_{w} / \tau)}{\sum_{k=1}^{K} exp(-D_{w} / \tau)},$$

本文将 \\(\tau\\) 固定为 0.17。  

训练第一阶段用 label smoothing，第二阶段改用软标签。  

## 分布可视化

![Fig 3](/images/2021/PRN103/3.png)
