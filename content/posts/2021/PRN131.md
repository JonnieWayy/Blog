---
title: "[论文阅读笔记 -- ReID] Learning To Know Where To See for Occluded ReID (ICCV 2021)"
date: 2021-11-30T11:50:29+08:00
categories: ["Paper Reading Notes"]
tags: ["paper reading", "cv", "notes", "cross-modal", "retrieval", "ReID"]
draft: false
---

# Learning To Know Where To See: A Visibility-Aware Approach for Occluded Person Re-Identification (ICCV 2021)

![Fig 1](/images/2021/PRN131/1.png)
![Fig 2](/images/2021/PRN131/2.png)

## 概述

随着姿态估计粒度的细化，其预测误差随之上升。  

本文试图找到一种策略，能在不过分依赖姿态信息的情况下处理遮挡问题，即使用粗粒度的姿态信息得到好的表现。  

首先学习一个局部标签生成器 (part label generator)，为各身体部件生成局部标签。基于生成的标签，学习一个区域可见性鉴别器 (region visibility discriminator)。  

再者，利用预训练好的姿态估计模型，得到姿态信息，包括其坐标与置信度。通过冗余投票 (redundant voting) 利用这些信息，决定一个部分是可见的还是被遮挡的。  

在测试阶段，可以直接使用学习好的区域可视性鉴别器，而无需姿态估计来得到可见性评分。  

![Fig 3](/images/2021/PRN131/3.png)

## Part Label Generaotr with a Redundant Voting

用 AlphaPose 作为 pose estimator，得到一组坐标及置信度 \\((cx_{j}, cy_{j}, s_{j}), \ j = 1, 2, \cdots, 18\\)。  

置信度表示一个关键点属于某个行人部件的几率，粗略而言，可用于表示一个部件是否可见。  

一个水平分割块可能包含多个关键点，本文设计了一种冗余投票方法 (redundant voting method)，将多个关键点的置信度转化为部件标签 (part label)。  

设定一个阈值 \\(\lambda\\)，若置信度大于阈值则相应关键点的投票权重为 1，否则为 0。最终对一个水平块中所有关键点的投票权重求和得到 \\(T\\)，若 \\(T\\) 高于某个阈值 \\(W\\) 则认为该水平块可见，其部件标签设为 1，否则被遮挡，部件标签设为 0。  

![Fig 4](/images/2021/PRN131/4.png)

## Region Visibility Discriminator

直接计算完整行人和遮挡行人之间的特征距离显然是不精准的，但是使用原始的姿态信息来得到可视化信息时，姿态信息又难以直接量化。  

旨在学习一个从粗糙的姿态信息到行人部件可视化评分的可靠映射。  

将各水平块的特征作为输入，估计其可视化评分，用 part label generator 生成的标签对其进行端到端优化。  

实现为 \\(1 \times 1\\) 卷积 + BN + FC + Sigmoid。  

## Visibility-Guided Constraint Loss

基于 batch hard triplet loss，其特征距离度量方式为：  

$$dist = \frac{\sum_{i=1}^{N}(l_{i}^q \cdot l_{i}^g) \cdot D(x_{i}^q, x_{i}^g) + D(F^q, F^g)}{\sum_{i=1}^{N} l_{i}^q \cdot l_{i}^g + 1},$$

其中 \\(l_{i}^q\\) 和 \\(l_{i}^g\\) 表示相应 query 或 gallery 图像第 \\(i\\) 个水瓶块的可视化评分。  

## 可视化评分示例

![Fig 5](/images/2021/PRN131/5.png)
