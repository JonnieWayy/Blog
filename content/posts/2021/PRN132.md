---
title: "[论文阅读笔记 -- 注意力机制] Learning LocFeat with Multiple Dynamic Attention (ICCV 2021)"
date: 2021-12-01T10:57:35+08:00
categories: ["Paper Reading Notes"]
tags: ["paper reading", "cv", "notes", "attention", "retrieval"]
draft: false
---

# Learning Deep Local Features With Multiple Dynamic Attentions for Large-Scale Image Retrieval (ICCV 2021)

[开源代码传送门](https://github.com/CHANWH/MDA)

![Fig 1](/images/2021/PRN132/1.png)

## 概述

由于图像内容的多样性，只提取一张注意力图难以有效捕捉所有潜在的语义模式。  

本文提出一个新架构，使用多个动态注意力来检测多样化的局部特征。

![Fig 2](/images/2021/PRN132/2.png)

## Multiple Dynamic Attention Module

旨在得到能够捕捉不同模式的注意力图。  

CNN 的特征图可以视为模式检测器 (pattern detector) 的 2D 响应图的集合，但某一视觉模式 (visual pattern) 相应的特征通道通常并不按顺序排列。  

对于一个特征图 \\(S\\)，首先用 \\(1 \times 1\\) 卷积对通道进行重排得到 \\(\hat{S}\\)，进而将其分为 \\(N\\) 组以表征多个独立的模式。  

将各组传入注意力生成模块 (attention generation module)，得到 \\(N\\) 个注意力图：  

$$F_{g}^i = \sigma(Conv_{1 \times 1}(AvgPool(F^i))),$$
$$A^i = Softplus(F_{g}^i \otimes F^i).$$

![Fig 3](/images/2021/PRN132/3.png)

## 训练阶段

### Contrasive Loss

利用注意力图池化各组局部特征得到一组全局特征，计算对比损失。  

### Diversity Regularization

旨在使得不同的注意力头学习到不同的注意力。  

首先将各注意力图 \\(A_{i}\\) 拉平为向量 \\(a_{i} \in \mathbb{R}^{H_{s}W_{s}}\\)，

$$\hat{a}\_{i} = softmax(a_{i}).$$

两个向量之间的 Hellinger 距离定义为：  

$$H(\hat{a}\_{i}, \hat{a}\_{j}) = \frac{1}{\sqrt{2}} ||\sqrt{\hat{a}\_{i}} - \sqrt{\hat{a}\_{j}}||\_{2}.$$

又因为 \\(\sum_{l=1}^{H_{s}W_{s}}\hat{a}\_{i, l} = 1\\)，有  

$$H(\hat{a}\_{i}, \hat{a}\_{j})^2 = 1 - <\sqrt{\hat{a}\_{i}}, \sqrt{\hat{a}\_{j}}>.$$

为了提高注意力图的多样性，应当最大化各向量之间的距离，因而将正则化项定义为：  

$$L_{reg} = \frac{1}{N(N - 1)} \sum_{i=1}^N \sum_{j \ne i}^N (<\sqrt{\hat{a}\_{i}}, \sqrt{\hat{a}\_{j}}> - 1).$$

## 测试阶段

测试阶段，根据注意力来选取 top-n 个局部特征。  

采用了二值化的聚合匹配核架构 ASMK\* 来整合多尺度的局部特征。  

![Fig 4](/images/2021/PRN132/4.png)

## 不同注意力模块对比

![Fig 6](/images/2021/PRN132/6.png)

## 注意力图可视化示例

![Fig 8](/images/2021/PRN132/8.png)
