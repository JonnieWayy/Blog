---
title: "[论文阅读笔记 -- ReID] Cross-Modal Knowledge Adaptation for LB Person Search (TIP 2021)"
date: 2021-12-25T14:59:38+08:00
categories: ["Paper Reading Notes"]
tags: ["paper reading", "cv", "notes", "cross-modal", "retrieval", "ReID"]
draft: false
---

# Cross-Modal Knowledge Adaptation for Language-Based Person Search (TIP 2021)

![Fig 1](/images/2021/PRN141/1.png)
![Fig 2](/images/2021/PRN141/2.png)

## 概述

本文着眼于公共空间学习时不同模态间表征的不一致性。

文本可用于指导图像特征丰富其重要的新人细节通知避免图像独有信息的干扰。  

本文提出一种跨模态知识自适应 (Cross-Modal Knowledge Adaptation, CMKA) 方法，从个体、列表及类别三个层面实现模态之间的知识自适应。  

![Fig 3](/images/2021/PRN141/3.png)

## CMKA

### 图文表征网络

图像模态用 ResNet-50，文本模态用 bi-LSTM。  

### 跨模态特征自适应

迫使图像表征适应于文本表征，最小化二者之间的距离：  

$$\mathcal{L}\_{FKA} = ||f_{i}^{Img} - f_{i}^{Txt}||^{2}.$$  

由于只要求图像特征适应于文本特征，该损失在模型训练时的反向传播不经过文本表征网络。  

### 跨模态排序列表自适应

在个体数据点的知识之外，利用排序列表的高阶知识。  

受 learning-to-rank 技术的启发，本文在不同模态之间进行扰动几率自适应 (permutation probability adaptation)。  

### 跨模态类几率自适应

基于知识蒸馏。  

### 模型测试

![Fig 4](/images/2021/PRN141/4.png)

## 检索结果对比示例

![Fig 6](/images/2021/PRN141/6.png)

## 特征可视化示例

![Fig 7](/images/2021/PRN141/7.png)

![Fig 8](/images/2021/PRN141/8.png)

![Fig 9](/images/2021/PRN141/9.png)
