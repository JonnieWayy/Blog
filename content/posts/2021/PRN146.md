---
title: "[论文阅读笔记 -- 自监督学习] Barlow Twins: SSL via Redundancy Reduction (ICML 2021)"
date: 2021-12-28T13:14:24+08:00
categories: ["Paper Reading Notes"]
tags: ["paper reading", "cv", "notes", "cross-modal", "retrieval", "ReID", "occuluded"]
draft: false
---

# Barlow Twins: Self-Supervised Learning via Redundancy Reduction (ICML 2021) 

[开源代码传送门](https://github.com/facebookresearch/barlowtwins)

![Fig 1](/images/2021/PRN146/1.png)

## 概述

自监督学习任务 (SSL)。  

现有工作的一个共同目标是要学习在不同扰动之下依旧稳定的表征，一种典型做法是最大化不同扰动下孪生网络得到的各表征之间的相似度，但这类方法面临着一个所谓无效解 (trivial solutions) 的问题，如得到一个常数表征等。  

此外还有对比方法、聚类方法等策略。  

本文提出一种新方法，称为 Barlow Twins，将 redundancy-reduction 理论应用于自监督学习。该理论假设感觉加工的目的在于将高度冗余的感觉输入重新编码为一个因子编码 (factorial code)，该编码有独立的统计成分。  

基于该理论，本文提出一种目标函数，旨在使得由一对孪生嵌入计算得到的互相关矩阵 (cross-correlation matrix) 尽可能接近于单位矩阵，其极其得利于超高维的嵌入。  

## Barlow Twins

对于所有图像，生成两个扰动后的视角，由一个数据增广分布 \\(\mathcal{T}\\) 得到，两笔数据由网络编码得到嵌入，并沿着 batch 维进行中心平均使得 batch 中各单元均值为 0。  

损失函数为：  

$$\mathcal{L}\_{\mathcal{BT}} = \sum_{i} (1 - \mathcal{C}\_{ii})^2 + \lambda \sum_{i}\sum_{j \ne i} \mathcal{C}\_{ij}^2,$$

其中前一项为 invariance 项，后一项为 redundancy reduction 项， \\(\mathcal{C}\\) 是沿着 batch 计算得到的两个相同网络输出之间的互相关矩阵：  

$$\mathcal{C}\_{ij} = \frac{\sum_{b} z_{b,i}^{A} z_{b,j}^{B}}{\sqrt{\sum_{b} (z_{b,i}^A)^2} \sqrt{\sum_{b} (z_{b,j}^B)^2}},$$

其中 \\(b\\) 是 batch 索引。  

## PyTorch 风格的伪代码

![Alg 1](/images/2021/PRN146/A1.png)
