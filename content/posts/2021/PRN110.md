---
title: "[论文阅读笔记 -- ReID] Knowledge-SV Learning: Knowledge Consensus Constraints (MM 2021)"
date: 2021-11-04T11:07:28+08:00
categories: ["Paper Reading Notes"]
tags: ["paper reading", "cv", "notes", "cross-modal", "retrieval", "ReID"]
draft: false
---

# Knowledge-Supervised Learning: Knowledge Consensus Constraints for Person Re-Identification (MM 2021)

![Fig 1](/images/2021/PRN110/1.png)

## 概述

本文旨在利用知识在不引入额外推断成本的基础上，约束同一数据上的多视角共识以提升精度。  

### 行人 ReID 相较于图像分类的特殊性
+ 检索任务，测试集 ID 与训练集不重叠，对模型泛化能力要求更高
+ 特征嵌入至关重要
+ 属于度量学习 (metric learning) 的一种

本文提出一种知识监督的学习方法 (Knowledge-Supervised Learning method, KSL)，在不引入额外推断成本的基础上提升表现。  

![Fig 2](/images/2021/PRN110/2.png)

## Isomorphic Auxiliary Training (AUX)

增加与主分支结构相同的辅助分支来提升精度，各分支采用交叉熵损失和三元组损失。  

值得注意的是在训练开始时直接加上辅助三元组损失会导致不收敛，文中通过在训练中期加入该损失解决这一问题。  

![Fig 3](/images/2021/PRN110/3.png)

## Knowledge Constraints by Dynamic Probability

### 基本想法
当模型能够最大化正确答案的平均对数几率，其也能够赋予所有错误答案几率，而错误答案的几率能够体现模型的泛化倾向，即借助动态分类几率的监督，能蒸馏出更多泛化信息。  

将预测到分类几率的相似度 (similarity of predicted classification probability, KSL-P) 作为 cosine 空间中的知识共识约束。采用两种形式：  
+ 密集的成对知识蒸馏 (densely pairwise knowledge distillation, KSL-PP) 用于保持多样性
+ 从有其他分支计算得到的虚拟平均分支蒸馏 (KSL-PA) 以过滤噪声

## Knowledge Constraints by Embedding Distance

将不同分支之间特征嵌入的距离 (KSL-E) 作为欧几里德空间中的知识共识约束，也采用与 KSL-P 中类似的 KSL-EP 和 KSL-EA 两种形式，但是没有方向。

## Knowledge Constraints of Hard Sample

前两种约束只考虑成对分支同一 ID 特征之间的知识传递，没有考虑不同分支不同 ID 之间的交互，因而引入困难样本互挖掘 (hard sample mutual mining, KSL-H) 作为困难样本空间中的知识共识约束。  

## 特征表征可视化示例

![Fig 7](/images/2021/PRN110/7.png)
