---
title: "[论文阅读笔记 -- ReID] TBPS in Full Images via Semantic-Driven Proposal Generation (2021)"
date: 2021-09-28T16:42:40+08:00
categories: ["Paper Reading Notes"]
tags: ["paper reading", "cv", "notes", "cross-modal", "retrieval", "ReID"]
draft: false
---

# 2109.12965 Text-based Person Search in Full Images via Semantic-Driven Proposal Generation

![Fig 1](/images/2021/PRN105/1.png)

## 概述

提出在完整图像中进行行人检索的任务，可视为 Person Detection 和 Text-based Person Retrieval 的结合。  

本文提出 Semantic-Driven Region Proposal Net (SDPRN)。  

构建了两个新数据集 CUHK-SYSU-TBPS 和 PRW-TBPS。  

![Fig 2](/images/2021/PRN105/2.png)

![Fig 3](/images/2021/PRN105/3.png)

![Fig 4](/images/2021/PRN105/4.png)

## 模型架构

将任务分解为三部分：  
+ 行人检测
+ 识别
+ 图文跨模态特征嵌入与匹配

### 语义驱动的 RPN (SDRPN)

基于 SENet，用语义特征调整 Base-Net 输出特征图的权重，采用通道注意力机制。  

### Proposal-Text 嵌入

旨在为图文模态学习一个公共空间，在嵌入过程中引入了跨尺度对齐策略 (cross-scale alignment scheme)。  

#### 多尺度视觉特征提取

三种尺度：  
+ global-scale
+ region-scale
+ local-scale

其中提取细粒度时用到了 Split & Shuffle 操作。  

#### 多尺度语义特征提取

用 BERT 提取三种尺度的特征：  
+ sentence-level
+ sub-sentence-level
+ word-level

#### Proposal-Text 跨尺度对齐

拼接各模态特征得到两组混合特征 (mixed visual/textual features)，记为 \\(I\\) 和 \\(T\\)，通过全连接层将 \\(I\\) 映射为 \\(Q, K, V\\)，将 \\(T\\) 映射为 \\(\mathcal{Q}, \mathcal{K}, \mathcal{V}\\)。  

由 T2V 注意力计算得到 attended semantic feature \\(A\\)：  

$$A = norm(Q\mathcal{K}^{T}) \odot \mathcal{V}.$$  

进而计算 \\(V\\) 和 \\(A\\) 之间的余弦相似度得到 \\(R\\)。图文对之间的相似度 \\(S\\) 通过对 \\(R\\) 中各部分求均值得到。  

类似方法可以求得 V2T 对之间的相似度 \\(S'\\)。  

归一化后的相似度 \\(\overline{S}\\) 可视为预测的匹配几率，而归一化的 0/1 匹配标签 \\(\overline{y}\\) 可视为分布标签，从而借助 KL 散度计算跨尺度对齐损失 (CSAL)：  

$$L_{CSAL} = L_{I} + L_{T} = D_{KL}(\overline{S}, \overline{y}) + D_{KL}(\overline{S'}, \overline{y'}).$$  

#### 全局匹配

使用 CMPM 损失和 CMPC 损失监督全局特征的跨模态匹配。  

## 结果示例

![Fig 6](/images/2021/PRN105/6.png)
