---
title: "[论文阅读笔记 -- ReID] Learning Domain Invariant Representations for Gen. ReID (2022)"
date: 2022-08-30T17:34:20+08:00
categories: ["Paper Reading Notes"]
tags: ["paper reading", "cv", "notes", "cross-modal", "retrieval", "ReID", "Domain Generalization"]
draft: false
---

# 2103.15890 Learning Domain Invariant Representations for Generalizable Person Re-identification (2022)

![Fig 1](/images/2022/PRN262/1.png)

## 概述

本文认为行人图像受以下两组潜在随机变量的影响：  
+ Identity-specific Factors \\(S\\)
+ Domain-specific Factors \\(V\\)

\\(V\\) 可能会混淆模型，两者间的伪相关也会使得模型难以基于 \\(S\\) 对 ID 标签作出可靠的预测。  

本文提出一种多域结构的对抗神经网络 (Multi-Domain Disentangled Adversarial Neural Network， MDDAN)，联合学习两个编码器，从多个 ReID 数据集中得到上述两种因子的嵌入。此外，一个后门调节模块 (Backdoor Adjustment block, BA) 被提出，用于估计介入性分布。二者整合在一起被称为 DIR-ReID。    

![Fig 2](/images/2022/PRN262/2.png)
![Fig 3](/images/2022/PRN262/3.png)
![Fig 4](/images/2022/PRN262/4.png)
