---
title: "[论文阅读笔记 -- 跨模态预训练] Democratizing Contrastive Language-Image Pre-training (2022)"
date: 2022-04-16T20:27:36+08:00
categories: ["Paper Reading Notes"]
tags: ["paper reading", "cv", "notes", "CLIP", "pretraining"]
draft: false
---

# 2203.05796 Democratizing Contrastive Language-Image Pre-training: A CLIP Benchmark of Data, Model, and Supervision (2022)

[开源代码传送门](https://github.com/Sense-GVT/DeCLIP)

![Tab 1](/images/2022/PRN221/T1.png)

## 概述

现有关于 CLIP 的工作由于其训练策略及所用数据的差异，难以公平对比其表现。  

本文旨在构建一个公平且可以复现的 CLIP 社区，对数据、监督、模型架构三个关键因素进行研究。  

![Fig 1](/images/2022/PRN221/1.png)
