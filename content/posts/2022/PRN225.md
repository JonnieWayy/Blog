---
title: "[论文阅读笔记 -- ReID] Lifelong Person ReID by Pseudo Task Knowledge Preservation (AAAI 2022)"
date: 2022-05-08T16:30:03+08:00
categories: ["Paper Reading Notes"]
tags: ["paper reading", "cv", "notes", "cross-modal", "retrieval", "ReID", "lifelong"]
draft: false
---

# Lifelong Person Re-identification by Pseudo Task Knowledge Preservation (AAAI 2022)

[开源代码传送门](https://github.com/g3956/PTKP)

![Fig 1](/images/2022/PRN225/1.png)

## 概述

Lifelong ReID 任务。  

### 两个主要困难
+ 面向任务的域鸿沟
+ 训练集与测试集的 ID 不重叠，与识别任务的终身学习不同，是一个开集问题

本文将 LReID 处理为域自适应问题，旧任务作为源域，新任务作为目标域，旨在缩小域鸿沟以学习任务共享的知识，提出了一种伪任务知识保留架构 (Pseudo Tasks Knowledge Preserving framework, PTKP)。  

![Fig 2](/images/2022/PRN225/2.png)

## Task-specific Domain Consistency Learning

本文提出应当对齐所有任务的分布，从而学习任务共享的知识，而非任务特定的知识。但是能够获取的源域样本很少。  

### Pseudo Task Transformation Module

旨在将新任务的特征映射到旧任务的特征空间，以弥补旧任务样本的不足。  

这里假设各任务服从一个任务特定的高斯分布，用 \\(S\\) 个任务特定的批归一化 (TSBN) 作为伪任务变换模块，其能够估计任务特定的统计量，并将特征映射到相应的任务特定特征空间中。  

### Domain Consistency Learning on Pseudo Tasks

本文在这里假设所有任务服从一个全局的高斯分布，用一个共享 BN 学习任务共享知识，进而最小化共享特征与伪任务特征之间的余弦相似度，以缩小面向任务的域鸿沟，称为 Task-specific Domain Consistency Loss。  

## Knoelwdge Preservation on Pseudo Tasks

上面的损失只考虑了新模型中的类内关系，忽略了类间关系和旧模型。  

### Pseudo Task Knowledge Distillation

在学习新知识的同时蒸馏旧知识到新模型中。  

提出一种伪任务知识蒸馏损失 (PT-KD)，最小化新旧模型提取的一个 batch 特征余弦相似度矩阵之间的 Frobenius 范数。  

### Pseudo Task Identity Discrimination

拉近 hard positive piar，推远 hard negative pair。  

## 可视化示例

![Fig 5](/images/2022/PRN225/5.png)
