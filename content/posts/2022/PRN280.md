---
title: "[论文阅读笔记 -- 迁移学习] Knowledge as Priors: CMKG for Datasets without SK (CVPR 2020)"
date: 2022-10-12T11:31:16+08:00
categories: ["Paper Reading Notes"]
tags: ["paper reading", "cv", "notes", "cross-modal", "teacher-student", "Domain Generalization", "Transfer Learning"]
draft: false
---

# Knowledge as Priors: Cross-Modal Knowledge Generalization for Datasets without Superior Knowledge (CVPR 2020)

![Fig 1](/images/2022/PRN280/1.png)

## 概述

本文基于跨模态知识蒸馏 (Cross-Modal Knowledge Distillation, CMKD) 任务，研究将从成对数据的源数据集上学习到的跨模态知识迁移到监督模态缺失的目标数据集上的可能性，提出跨模态知识泛化 (Cross-Modal Knowledge Generalization, CMKG) 任务。  

### CMKD 架构

![Fig 2](/images/2022/PRN280/2.png)

### CMKG 架构

![Fig 3](/images/2022/PRN280/3.png)

![Alg 1](/images/2022/PRN280/A1.png)

![Alg 2](/images/2022/PRN280/A2.png)
