---
title: "[论文阅读笔记 -- 域泛化] Learning to Diversify for Single Domain Generalization (ICCV 2021)"
date: 2022-07-11T15:25:27+08:00
categories: ["Paper Reading Notes"]
tags: ["paper reading", "cv", "notes", "cross-modal", "domain generalization"]
draft: false
---

# Learning to Diversify for Single Domain Generalization (ICCV 2021)

[开源代码传送门](https://github.com/BUserName/Learning_to_diversify)

## 概述

主流的 DG 技术可分为基于对齐的和基于增广的两种。  

本文关注只有单个源域的域泛化问题，提出一种 Learning-to-diversify (L2D) 方法。  

![Fig 1](/images/2022/PRN254/1.png)

## 本文方法

### Style-Complement Module

包含 K 种变换，各由一个卷积层、一个风格学习层和一个转置卷积层构成，各维护一组可学习参数 \\(\theta_{G, k} = \\{\mu_{k}, \sigma_{k}\\}\\)。  

风格学习层实现为：  

$$T(f_{i, k}; \theta_{G, k}) = \sigma_{k} \* \frac{f_{i, k} - \mu}{\sigma} + \mu_{k}.$$

最终输出为 K 个增广后图像的线性组合：  

$$x_{i}^{+} = \frac{1}{\sum_{k=1}^K(w_{k})} \sum_{k=1}^K (w_{k} tanh(x_{i, k}^{+})),$$
$$w_{k} \sim Normal(0, 1).$$

### Synthesizing Novel Styles

为了提升生成图像风格的多样性，应当最小化生成图像与原始图像之间的相关性，本文用互信息来建模相关性。  

将二者在潜在特征空间中互信息的上界定义为：  

$$\hat{I}(z; z^{+}) = \frac{1}{N} \sum_{i=1}^N [log q_{\theta}(z_{i}^{+} | z_{i}) - \frac{1}{N} \sum_{j=1}^N log q_{\theta}(z_{j}^{+} | z_{i})].$$

### Semantic Consistency

风格补充模块可能会引入噪声信息。  

因此本文在这里最小化潜在空间中的 class-conditional maximum mean discrepancy (MMD)。  

### MI Maximization

旨在平衡风格补充模块和主模型。  
