---
title: "[论文阅读笔记 -- 跨模态预训练] LiT: Zero-Shot Transfer with Locked-image text Tuning (2022)"
date: 2022-04-24T16:44:47+08:00
categories: ["Paper Reading Notes"]
tags: ["paper reading", "cv", "notes", "cross-modal", "CLIP", "pretraining"]
draft: false
---

# 2111.07991 LiT: Zero-Shot Transfer with Locked-image text Tuning (2022)

![Fig 1](/images/2022/PRN222/1.png)

## 概述

零射迁移 (zero-shot transfer)，不同于传统零射学习，其预训练过程中看不到迁移相关的监督信息。  

本文提出采用对比学习架构，提出一种对比微调 (contrastive-tuning) 策略，称为 Locked-image Tuning (LiT)，用预训练的视觉模型结合图文数据来微调文本模型，视觉模型锁定得到的表现最好。  

本文认为 LiT 表现好的原因在于学习图像描述器与图文对齐时数据与技术的分离。图文数据能有助于学习自然语言与视觉世界之间的相关性，但可能不够精确。  

![Fig 2](/images/2022/PRN222/2.png)

## 本文方法

### 对比预训练

对比损失的一个重要细节在于其是在各加速设备上独立计算后聚合还是在所有设备上联合计算，实验表明后者表现更好。  

### 对比微调

对比预训练可以视为同时学习两个任务：  
+ 学习图像嵌入
+ 学习与图像嵌入空间对齐的文本嵌入

除了成对预训练之外学习图像嵌入的一种方法是利用大规模标注的图像数据集，但这种方法受限于有限的类别。图文数据就没有这样的限制，但是可能数据质量更差。  

本文采用的对比微调就是旨在结合两种源数据的优点。  
