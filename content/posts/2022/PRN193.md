---
title: "PRN193"
title: "[论文阅读笔记 -- ASR / 终身学习] Towards Lifelong Learning of End-to-end ASR (2021)"
date: 2022-02-19T21:06:23+08:00
categories: ["Paper Reading Notes"]
tags: ["paper reading", "cv", "notes", "lifelong learning", "asr"]
draft: false
---

# 2104.01616 Towards Lifelong Learning of End-to-end ASR (2021)

![Fig 1](/images/2022/PRN193/1.png)

## 概述

终身学习 (LLL) 可分为三类：  
+ Regularization-based Methods: 通过在损失函数中加入正则化项来加固模型中的关键参数
+ Architecture-based Methods: 为各任务分配一定的模型容量，或扩展模型以处理额外任务
+ Data-based Methods: 保留或生成一些来自过去任务的样本

本文将 LLL 应用于自动语音识别 (ASR) 任务上。  

## Regularization-based Methods

需要存储先前模型的权重，占据较大空间。  

### Elastic Weight Consilidation (EWC)

采用 EWC 和 online EWC 算法，用一个正则化项约束模型参数和先前任务保持接近：  

$$\mathcal{L}(\theta^k) = \mathcal{L}\_{CTC}(\theta^k) + \frac{\lambda}{2} \sum_{i} \Omega_{i}^{k} (\theta\_{i}^k - \theta\_{i}^{k-1^{\*}})^2.$$

### Synaptic Intelligence (SI)

和 EWC 相似，但是权重由对应项对于损失下降的作用递归更新得到：  

$$\Omega_{i}^k = \Omega_{i}^{k-1} + \frac{\omega_{i}^{k-1}}{(\theta\_{i}^{k-1^{\*}} - \theta\_{i}^{k-2^{\*}})^2 + \xi},$$

其中 \\(\omega\\) 由 CTC 损失的梯度得到。  

### Knowledge Distillation (KD)

约束新旧模型的输出分布，而非直接约束模型参数。  

## Data-based Methods

### Gradient Episodic Memory (GEM)

保留过往样本以计算梯度，若当前梯度 \\(g\\) 导致在任何先前域上的损失上升，则对其进行投影：  

$$minimize_{\widetilde{g}} \quad ||g - \widetilde{g}||\_{2}^2,$$
$$s.t. \quad <\widetilde{g}, g_{k-1}> \ge 0.$$

本文为每个域保留相同数量的样本，并固定存储总量，所以有新任务时需要丢弃一部分旧样本，文章提出两种数据选取方法。  

### Minimum Perplexity (PP)

为每个数据集训练一个 LM，保留最能表征该数据集的复杂度最小的发言。  

### Median Length (Len)

由于各数据集的平均发言长度不一样，保留长度最接近中位数的发言。  
