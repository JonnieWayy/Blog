---
title: "[论文阅读笔记 -- ReID] Hashing Person Re-ID with Self-distilling Smooth Relaxation (NC 2021)"
date: 2022-01-30T22:02:51+08:00
categories: ["Paper Reading Notes"]
tags: ["paper reading", "cv", "notes", "cross-modal", "retrieval", "ReID", "hashing"]
draft: false
---

# Hashing Person Re-ID with Self-distilling Smooth Relaxation (Neurocomputing 2021)

![Fig 2](/images/2022/PRN181/2.png)

## 概述

影响哈希 ReID 表现的两大因素：
+ 特征表征：与非哈希方法的浮点特征相比，哈希编码在表征能力上具有内在的劣势
+ 哈希松弛度：为了能够优化，训练阶段模型的输出被松弛化为近似二进制的实数值，但是该过程会导致信息损失

此外，缩短编码长度也会导致表现的下降。  

行人 ReID 任务训练集与测试集中的 ID 信息不重叠，但其行人属性信息是相同的，因此本文考虑利用属性预测，在 ID 特征之外提供不同粒度的有效描述。  

本文提出一种自蒸馏的快速哈希 ReID 架构 (Self-distilled Fast Hashing ReID Framework)。  

其中，提出一种基于属性的快速检索策略 (Attribute-based Fast Retrieval Strategy, AFR) 来加速检索，以二分类的形式学习行人属性，作为粗粒度的身份信息。  

此外，提出一种名为自蒸馏平滑松弛 (Self-distilling Smooth Relaxation, SSR) 的二值编码学习方法，以充分挖掘深度特征来生成哈希编码。  

![Fig 1](/images/2022/PRN181/1.png)

## Quantized Semantic Self-distillation

### 全局-局部表征

选择 ResNet-50 作为 backbone，选用 PCB 作为基本网络得到粗略的局部表征，特征图水平 6 等分。  

分为 teacher binary descriptor 和 student relaxed descriptor 两个分支，分别得到全局和局部特征。  

### 自蒸馏平滑松弛 (SSR)

为了能够直接优化，采用实数值特征，但由于欧几里德空间和汉明空间的差异，基于实数值学习到的特征并不能得到最优的检索表现。  

因此需要用到松弛化，即在分类器之间加上一个隐层，通常为非线性激活层，如 Sigmoid、Tanh 或 Hardsigmoid等，而在测试阶段则直接将松弛化后的特征量化为二值编码。  

但是在压缩特征值分布范围的同时，这类方法也会压缩特征的表征空间，从而造成信息损失。  

本文提出 SSR，教师描述器首先在没有松弛化的情况下量化为二值哈希编码，大于 0 为 1，否则为 -1。接着用其将二值语义知识迁移到学生描述器，计算与其实值哈希编码之间的 L1 损失。全局与局部特征拼接后进行。  

![Fig 3](/images/2022/PRN181/3.png)

## 基于属性的快速检索 (AFR)

### 二元属性学习与快速检索

为了加速检索，用二分类属性学习取代分类属性学习，将各属性多个值的预测变为多个二分类任务，用到二元交叉熵损失进行训练。  

测试时包含两个阶段：  
+ fast attribute coarse selection
+ ID features-based search

预测到的属性几率被量化而属性二值编码，学生网络的输出特征被量化而身份二值编码，二者共同用于检索。  

首先用属性编码计算汉明距离，从 gallery 选出前 \\(w\\) 个候选，接着用身份编码计算汉明距离进行二次排序。  

### 属性指导的注意力块 (AAB)

属性与 ID 学习关注不同的知识粒度，简单结合反而影响 ID 特征的表现，因此本文不仅在损失层面，也在特征层面融合属性与身份的识别任务，利用空间和通道注意力机制。  

![Fig 4](/images/2022/PRN181/4.png)

## 损失函数

本文架构涉及的学习任务包含身份识别、属性识别、自蒸馏平滑松弛化和尺度学习。  

尺度学习用到困难样本挖掘的三元组损失。  

身份和属性识别用到交叉熵损失。  

自蒸馏平滑松弛化用到 L1 损失。  
