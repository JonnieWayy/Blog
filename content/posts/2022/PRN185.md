---
title: "[论文阅读笔记 -- ReID] Cross-Modal Cross-Domain Moment Alignment Network (CVPR 2020)"
date: 2022-02-11T15:51:42+08:00
categories: ["Paper Reading Notes"]
tags: ["paper reading", "cv", "notes", "cross-modal", "retrieval", "ReID", "Domain"]
draft: false
---

# Cross-Modal Cross-Domain Moment Alignment Network for Person Search (CVPR 2020)

![Fig 1](/images/2022/PRN185/1.png)

## 概述

本文最早提出跨模态跨域的文本行人检索任务，设计了一种时刻对齐网络 (Moment Alignment Network, MAN)，其包含三种对齐模块：  
+ Domain Alignment (DA)
+ Cross-modal Alignment (CA)
+ Exemplar Alignment (EA)

![Fig 2](/images/2022/PRN185/2.png)

## 本文方法

### Class Moments

分别用 ResNet-50 和 bi-LSTM 提取图文特征，分别为源域/目标域中的图文数据学习四个分类器。  

与通常的 UDA 任务不同，行人检索的类别更多，一个 mini-batch 无法包含全部。因此，本文用分类器参数表征数据分布的均值，称为 class mean，并由其计算方差。  

本文认为源域文本与目标域文本之间不存在域迁移。  

均值和方差在对其过程中被称为 class moments。  

对于没有标签的目标域样本，用 self-labelling 生成其伪标签：  

$$y_{I_{k}}^{t} = Softmax(cos(f(I^t), \mu_{I_{k}}^s)),$$
$$y_{T_{k}}^{t} = Softmax(cos(f(T^t), \mu_{T_{k}}^s)).$$

### Domain Alignment

本文认为域迁移主要来自行人外观分布的差异，而不同数据集之间词汇差异较小。  

本文进行类级别的域对齐，而非全局分布级别：  

$$L_{CDC} = \sum_{k=1}^C d(\mu_{I_{k}}^s, \mu_{I_{k}}^t) + \gamma_{1} d(\sigma_{I}^s, \sigma_{I}^t).$$

### Cross-Modal Alignment

$$L_{CMC} = \sum_{k=1}^C d(\mu_{I_{k}}^t, \mu_{T_{k}}^t) + \gamma_{1} d(\sigma_{I}^t, \sigma_{T}^t).$$

源域上用到三元组损失和身份损失。  

### Exemplar Alignment

类似交叉熵损失，使得各样本与最近的类别均值接近。  

## 数据集

![Fig 3](/images/2022/PRN185/3.png)

## 特征可视化示例

![Fig 4](/images/2022/PRN185/4.png)
