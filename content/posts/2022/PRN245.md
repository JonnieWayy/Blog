---
title: "[论文阅读笔记 -- 跨模态检索] Hierarchical VT KD for Life-Long Correlation Learning (IJCV 2021)"
date: 2022-06-22T15:31:35+08:00
categories: ["Paper Reading Notes"]
tags: ["paper reading", "cv", "notes", "cross-modal", "retrieval", "lifelong"]
draft: false
---

# Hierarchical Visual-Textual Knowledge Distillation for Life-Long Correlation Learning (IJCV 2021)

![Fig 1](/images/2022/PRN245/1.png)

## 概述

本文提出一种视觉-文本终身知识蒸馏方法 (Visual-textual Life-long Knowledge Distillation approach, VLKD)，在语义与注意力级别利用从现有数据中学习到的知识。  

### 核心思路

![Fig 2](/images/2022/PRN245/2.png)

### 模型架构

![Fig 3](/images/2022/PRN245/3.png)

### Adaptive Network Expansion

![Fig 4](/images/2022/PRN245/4.png)
