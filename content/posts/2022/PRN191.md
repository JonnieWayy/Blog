---
title: "[论文阅读笔记 -- 跨模态检索] Adaptive CM Prototypes for Cross-Domain VLR (CVPR 2021)"
date: 2022-02-15T22:09:12+08:00
categories: ["Paper Reading Notes"]
tags: ["paper reading", "cv", "notes", "cross-modal", "retrieval", "cross-domain"]
draft: false
---

# Adaptive Cross-Modal Prototypes for Cross-Domain Visual-Language Retrieval (CVPR 2021)

![Fig 1](/images/2022/PRN191/1.png)

## 概述

将在带标签源域上学习到的模型迁移到不带标签的目标域的任务被称为无监督域自适应 (UDA)，本文研究 UDA 设定下的跨模态检索问题，主要面临三大挑战：  
+ Compositionality: 模型需要对多种视觉目标的组合形式进行语义编码
+ Reporting Bias: 文本通常不会描述到图中所有的细节
+ Visual And Text Domain Shifts: 不同域中的图像与文本数据都可能存在较大的差异

本文提出一种自适应跨模态原型架构 (Adaptive Cross-modal Prototypes framework, ACP)。  

![Fig 2](/images/2022/PRN191/2.png)

## 本文方法

### Visual and Textual Encoders

预训练的编码器分别处理图文数据，利用源域的成对数据，用双向三元组损失训练。  

### Visual and Textual Keels

利用各模态中已经可获取的结构化知识，构建图文 keel，以表征多概念组合后复杂的语义特征。  

首先用预训练的 generic visual and text descriptors 独立构建单模态数据结构，用 Lloyd 算法聚类各得到一组中心，称为图像或文本 keel。  

### Source and Target Prototypical Network


