---
title: "[论文阅读笔记 -- 跨模态检索] Continual Learning in Cross-modal Retrieval (CVPRW 2021)"
date: 2022-02-13T15:51:47+08:00
categories: ["Paper Reading Notes"]
tags: ["paper reading", "cv", "notes", "cross-modal", "retrieval", "continual learning"]
draft: false
---

# Continual Learning in Cross-modal Retrieval (CVPRW 2021)

![Fig 1](/images/2022/PRN187/1.png)

## 概述

本文将终身学习与跨模态检索相结合。  

终身学习的一个困境在于所谓的灾难性遗忘 (catastrophic forgetting)，当前一种主流方法是给损失加上正则化项，另一种是复述 (rehearsal) 或伪复述 (pseudo-rehearsal)，即保留先前任务的一部分数据参与新的训练，后者则用辅助模型对先前任务建模。  

## 三个阶段

### Training (Feature Extractors)

双分支结构分别提取图文特征，训练得到其优化后的模型参数。  

### Indexing (Database Data)

用训练好的网络处理数据库数据，得到特征。  

本文为了简化，将数据库数据直接用作训练数据。  

### Querying (Query Data)

计算 query 样本与数据库特征的相似度，排序后得到目标样本。  

![Fig 2](/images/2022/PRN187/2.png)

## A Framework for Continual Retrieval

将数据表征为一个任务序列，各任务涉及不同域的数据。  

![Fig 3](/images/2022/PRN187/3.png)

## Do or Do Not Reindex?

### Reindexing

当前任务以及所有先前任务用新版本的模型得到新的特征，缺点是时间和计算成本高，而且需要一直能获取先前任务的数据，优点是数据库样本和 query 样本由相同的网络进行处理。  

### No Reindexing

更高效，但引入了不对称性。  

![Fig 4](/images/2022/PRN187/4.png)

## 跨模态嵌入的灾难性遗忘

本文讨论了会导致这一现象的几种因素。  

### Embedding Networks

对于先前任务而言，模型参数从最优值位置发生了偏移，自然导致了表现的下降。  

### Embedding Misalignment

不同模态的嵌入特征可能会在新任务训练过程中朝着不同方向偏移，而基于孪生网络或三元组网络的单模态检索模型并不会遇到该问题。  

### Task Overlap

由于训练一个任务时先前任务的负样本并不可见，不同任务的样本可能会在嵌入空间中发生重叠，不过在 query 阶段其他任务的数据并不会被考虑。  

## 遗忘的预防

### Preventing Embedding Drift

在损失函数上加一个正则化项，约束两步参数之间的差异。  

### Preventing Unequal Drift

共享最后一层的网络参数。  

### Decoupling Retrieval Directions

在 no reindexing 设定下，可以分别为 i2t 和 t2i 两个方向各训练一个模型。  

## 可视化示例

![Fig 5](/images/2022/PRN187/5.png)
